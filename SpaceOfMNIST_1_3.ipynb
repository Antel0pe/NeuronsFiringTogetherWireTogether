{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ0kSdmm4--I",
        "outputId": "6a2b38f5-0242-4ecb-8777-1d53c36dc207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✨🍰✨ Everything looks OK!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "# Install all your required packages here using mamba\n",
        "!mamba install -q scikit-learn graph-tool"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Subset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the transformation pipeline:\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: (x > 0.5).float()),  # Binarize the image\n",
        "    transforms.Lambda(lambda x: x.view(-1))           # Flatten into a 784-dim vector\n",
        "])\n",
        "\n",
        "# Load the training set (set download=True if running for the first time)\n",
        "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Get indices for images where the label is 1\n",
        "indices = (mnist_train.targets == 1).nonzero().squeeze()\n",
        "\n",
        "# Create a subset containing only the '1's\n",
        "mnist_train_ones = Subset(mnist_train, indices)\n",
        "\n",
        "print(f\"Total number of '1' images in the training set: {len(mnist_train_ones)}\")\n",
        "\n",
        "# Stack all 784-dim vectors from the filtered dataset\n",
        "all_vectors = torch.stack([img for img, _ in mnist_train_ones])\n",
        "unique_vectors = torch.unique(all_vectors, dim=0)\n",
        "\n",
        "print(f\"Total images in mnist_train_ones: {all_vectors.shape[0]}\")\n",
        "print(f\"Unique images: {unique_vectors.shape[0]}\")\n",
        "\n",
        "if all_vectors.shape[0] == unique_vectors.shape[0]:\n",
        "    print(\"All 784-dimensional vectors are unique.\")\n",
        "else:\n",
        "    print(\"There are duplicates in the 784-dimensional vectors.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p1byeYZ5KEx",
        "outputId": "9f378364-0859-4109-b43e-d2698e674aee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of '1' images in the training set: 6742\n",
            "Total images in mnist_train_ones: 6742\n",
            "Unique images: 6726\n",
            "There are duplicates in the 784-dimensional vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import graph_tool as gt\n",
        "\n",
        "def visualize_threshold_graph_gt_weighted(unique_vectors, distance_threshold):\n",
        "    \"\"\"\n",
        "    Builds a weighted graph with graph-tool where each unique vector is a node and an edge is added\n",
        "    between nodes if their Euclidean distance is <= distance_threshold.\n",
        "    The edge is weighted by the real Euclidean distance between the nodes.\n",
        "\n",
        "    Args:\n",
        "        unique_vectors (torch.Tensor or np.array): Array/tensor of shape (n, 784) containing unique images.\n",
        "        distance_threshold (float): Maximum Euclidean distance for adding an edge between two nodes.\n",
        "\n",
        "    Returns:\n",
        "        g (graph_tool.Graph): The constructed weighted graph.\n",
        "    \"\"\"\n",
        "    # Convert tensor to numpy array if necessary.\n",
        "    X = unique_vectors\n",
        "\n",
        "    n = X.shape[0]\n",
        "    print(f\"Building weighted graph for {n} nodes...\")\n",
        "\n",
        "    # Use NearestNeighbors to find all pairs within the distance threshold.\n",
        "    nbrs = NearestNeighbors(radius=distance_threshold, algorithm='auto').fit(X)\n",
        "    distances, indices = nbrs.radius_neighbors(X)\n",
        "\n",
        "    # Create an undirected graph using graph-tool.\n",
        "    g = gt.Graph(directed=False)\n",
        "    g.add_vertex(n)  # Add n vertices\n",
        "\n",
        "    # Build a list of edges with weights\n",
        "    edge_list = []\n",
        "    for i, (dists, neigh) in enumerate(zip(distances, indices)):\n",
        "        # For each neighbor j of node i, add edge only if i < j to avoid duplicates.\n",
        "        edge_list.extend([[i, j, d] for d, j in zip(dists, neigh) if i < j])\n",
        "\n",
        "    # Convert the list of edges to a NumPy array (with float type to hold the weight)\n",
        "    edge_array = np.array(edge_list)\n",
        "\n",
        "    eweight = g.new_ep(\"float\")\n",
        "    # Add all edges at once\n",
        "    g.add_edge_list(edge_array, eprops=[eweight])\n",
        "\n",
        "    print(f\"Weighted graph built with {g.num_edges()} edges.\")\n",
        "    return g\n",
        "\n",
        "# Example usage:\n",
        "g = visualize_threshold_graph_gt_weighted(unique_vectors, distance_threshold=6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCK9ZYEI5R-r",
        "outputId": "18201f1d-9e03-460b-ad8b-354b8ce16a4d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building weighted graph for 6726 nodes...\n",
            "Weighted graph built with 4681902 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import graph_tool.all as gt\n",
        "import graph_tool.topology as gt_topo\n",
        "\n",
        "def find_furthest_nodes_double_sweep_largest_component(g, X):\n",
        "    \"\"\"\n",
        "    For a graph g (built with graph-tool) and an array X of shape (n, 784) corresponding to the\n",
        "    unique vectors (with row i corresponding to vertex i), this function:\n",
        "      1. Adds a vertex property to store original indices.\n",
        "      2. Extracts the largest connected component.\n",
        "      3. Finds two nodes that are farthest apart (approximate diameter endpoints) using a double-sweep algorithm\n",
        "         on the subgraph.\n",
        "      4. Computes the shortest path between them in the subgraph.\n",
        "      5. Maps the subgraph vertices back to the original graph using the stored property.\n",
        "      6. Extracts the corresponding 784-d vectors and computes delta vectors along the path.\n",
        "\n",
        "    Returns:\n",
        "      endpoints_sub: Tuple with the two endpoint indices in the subgraph (original indices).\n",
        "      path_indices: List of original graph vertex indices along the shortest path.\n",
        "      path_vectors: Array of the corresponding 784-d vectors for each vertex on the path.\n",
        "      path_deltas: List of delta vectors between consecutive nodes on the path.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Store original vertex indices as a vertex property ---\n",
        "    orig_index = g.new_vertex_property(\"int\")\n",
        "    for v in g.vertices():\n",
        "        orig_index[v] = int(v)\n",
        "\n",
        "    # --- Step 2: Extract the largest connected component ---\n",
        "    comp, hist = gt.label_components(g)\n",
        "    largest_comp = np.argmax(hist)\n",
        "    vfilt = comp.a == largest_comp\n",
        "    g_sub = gt.GraphView(g, vfilt=vfilt)\n",
        "\n",
        "    # Build a mapping from subgraph vertex object to original graph index.\n",
        "    vertex_to_orig = {v: int(orig_index[v]) for v in g_sub.vertices()}\n",
        "    # Also, get a list of subgraph vertices.\n",
        "    vertices_sub = list(g_sub.vertices())\n",
        "    n_sub = len(vertices_sub)\n",
        "    print(f\"Largest connected component has {n_sub} vertices (out of {int(g.num_vertices())} total).\")\n",
        "\n",
        "    # --- Step 3: Double Sweep Algorithm on the subgraph ---\n",
        "    # Use the list of vertices instead of range(n_sub)\n",
        "    v0 = vertices_sub[0]\n",
        "    dmap = gt.shortest_distance(g_sub, source=v0)\n",
        "    # Find the vertex farthest from v0 among vertices_sub.\n",
        "    farthest_idx = np.argmax([dmap[v] for v in vertices_sub])\n",
        "    v1 = vertices_sub[farthest_idx]\n",
        "\n",
        "    dmap2 = gt.shortest_distance(g_sub, source=v1)\n",
        "    farthest_idx2 = np.argmax([dmap2[v] for v in vertices_sub])\n",
        "    v2 = vertices_sub[farthest_idx2]\n",
        "\n",
        "    endpoints_sub = (vertex_to_orig[v1], vertex_to_orig[v2])\n",
        "    diameter_length = dmap2[v2]\n",
        "    print(\"Diameter endpoints in subgraph (original indices):\", endpoints_sub, \"with length:\", diameter_length)\n",
        "\n",
        "    # --- Step 4: Compute the shortest path in the subgraph between v1 and v2 ---\n",
        "    path_vertices_sub, _ = gt_topo.shortest_path(g_sub, source=v1, target=v2)\n",
        "    # path_vertices_sub is a list of vertex objects in g_sub.\n",
        "    print(\"Shortest path in subgraph (vertex objects):\", list(path_vertices_sub))\n",
        "\n",
        "    # --- Step 5: Map subgraph vertex objects back to original graph indices ---\n",
        "    path_indices = [vertex_to_orig[v] for v in path_vertices_sub]\n",
        "    print(\"Mapped original graph vertex indices for path:\", path_indices)\n",
        "\n",
        "    # --- Step 6: Extract the corresponding 784-d vectors and compute delta vectors ---\n",
        "    path_vectors = X[path_indices]\n",
        "    path_deltas = [path_vectors[i+1] - path_vectors[i] for i in range(len(path_vectors) - 1)]\n",
        "\n",
        "    return endpoints_sub, path_indices, path_vectors, path_deltas\n",
        "\n",
        "# Example usage:\n",
        "# Assume unique_vectors is your NumPy array of shape (n, 784)\n",
        "endpoints_sub, path_indices, path_vectors, path_deltas = find_furthest_nodes_double_sweep_largest_component(g, unique_vectors)\n",
        "\n",
        "print(\"\\n--- Results ---\")\n",
        "print(\"Furthest endpoints in subgraph (original indices):\", endpoints_sub)\n",
        "print(\"Path indices (original graph):\", path_indices)\n",
        "print(\"Path vectors shape:\", np.array(path_vectors).shape)\n",
        "print(\"Number of delta vectors:\", len(path_deltas))\n"
      ],
      "metadata": {
        "id": "foXefCRE5gbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_path_as_images(path_vectors):\n",
        "    \"\"\"\n",
        "    Given an array of 784-d vectors (each representing an MNIST image),\n",
        "    reshapes them into 28x28 images and plots them side-by-side.\n",
        "\n",
        "    Args:\n",
        "        path_vectors (np.array): Array of shape (n_steps, 784)\n",
        "    \"\"\"\n",
        "    num_steps = len(path_vectors)\n",
        "    # Create a figure with one subplot per image along the path.\n",
        "    fig, axs = plt.subplots(1, num_steps, figsize=(num_steps * 2, 2))\n",
        "\n",
        "    # If only one image, ensure axs is iterable.\n",
        "    if num_steps == 1:\n",
        "        axs = [axs]\n",
        "\n",
        "    for i, vec in enumerate(path_vectors):\n",
        "        img = vec.reshape(28, 28)\n",
        "        axs[i].imshow(img, cmap='gray')\n",
        "        axs[i].axis(\"off\")\n",
        "        axs[i].set_title(f\"Step {i}\", fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Assuming path_vectors is the output from the previous function\n",
        "plot_path_as_images(path_vectors)\n"
      ],
      "metadata": {
        "id": "r85EPspZ5-ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assume path_deltas is a list of 10 arrays, each of shape (784,)\n",
        "# First, stack them into a 2D array with shape (10, 784)\n",
        "deltas_array = np.stack(path_deltas)  # Shape: (10, 784)\n",
        "\n",
        "# Transpose the array to get shape (784, 10)\n",
        "deltas_by_dimension = deltas_array.T  # Each row now corresponds to a dimension\n",
        "\n",
        "# If you need a list of 784 arrays (each of length 10)\n",
        "deltas_list_by_dimension = [deltas_by_dimension[i, :] for i in range(deltas_by_dimension.shape[0])]\n",
        "\n",
        "print(\"Shape of deltas_array (by path step):\", deltas_array.shape)\n",
        "print(\"Shape of deltas_by_dimension (by dimension):\", deltas_by_dimension.shape)\n"
      ],
      "metadata": {
        "id": "lbPG2ry-6cFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming deltas_by_dimension is a NumPy array of shape (784, 10)\n",
        "indices_with_one = np.where(np.any(deltas_by_dimension != 0, axis=1))[0]\n",
        "print(\"Indices with at least one 1:\", indices_with_one)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming deltas_by_dimension is a numpy array of shape (784, 10)\n",
        "# Compute the count of 1s for each row\n",
        "ones_count = np.sum(deltas_by_dimension != 0, axis=1)\n",
        "\n",
        "# Create a DataFrame with row numbers and count of 1s\n",
        "df = pd.DataFrame({\n",
        "    'Row': np.arange(deltas_by_dimension.shape[0]),\n",
        "    \"Count of 1's\": ones_count\n",
        "})\n",
        "\n",
        "# Sort the DataFrame in descending order by the count of 1s\n",
        "df_sorted = df.sort_values(by=\"Count of 1's\", ascending=False)\n",
        "\n",
        "# Display the sorted table\n",
        "print(df_sorted)\n"
      ],
      "metadata": {
        "id": "OCgxCWCM79Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import graph_tool.all as gt\n",
        "\n",
        "# Label connected components; comp is a property map with component labels,\n",
        "# and hist contains the sizes of each component.\n",
        "comp, hist = gt.label_components(g)\n",
        "\n",
        "# Identify the label corresponding to the largest component.\n",
        "largest_component_label = np.argmax(hist)\n",
        "\n",
        "# Create a vertex filter that is True for vertices in the largest component.\n",
        "vfilt = comp.a == largest_component_label\n",
        "\n",
        "# Create a subgraph view using the vertex filter.\n",
        "g_lcc = gt.GraphView(g, vfilt=vfilt)\n",
        "\n",
        "print(\"Largest connected component has\", g_lcc.num_vertices(), \"vertices and\", g_lcc.num_edges(), \"edges.\")\n"
      ],
      "metadata": {
        "id": "2VOBIjwm8qU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume g is your original graph and it has a vertex property \"orig_index\"\n",
        "orig_index_prop = g.vertex_properties[\"orig_index\"]\n",
        "\n",
        "# Build a mapping for vertices in the connected component\n",
        "vertex_to_orig = {v: int(orig_index_prop[v]) for v in g_lcc.vertices()}\n"
      ],
      "metadata": {
        "id": "EMnGWVS_jub4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a TensorDataset using the unique vectors\n",
        "unique_dataset = TensorDataset(unique_vectors)\n",
        "\n",
        "# Define a simple autoencoder architecture\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim=32):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder: 784 -> 256 -> 128 -> latent_dim\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(784, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, latent_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Decoder: latent_dim -> 128 -> 256 -> 784\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 784),\n",
        "            nn.Sigmoid()  # Output between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        reconstructed = self.decoder(z)\n",
        "        return reconstructed\n",
        "\n",
        "    def get_latent(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 256\n",
        "num_epochs = 50\n",
        "learning_rate = 1e-3\n",
        "latent_dim = 32\n",
        "\n",
        "# Create DataLoader for the unique dataset\n",
        "unique_loader = DataLoader(unique_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Set up device and model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Autoencoder(latent_dim=latent_dim).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Use binary cross entropy since inputs are binary\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop for the autoencoder on unique vectors\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in unique_loader:\n",
        "        images = batch[0].to(device)  # images shape: [batch_size, 784]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, images)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    epoch_loss = running_loss / len(unique_loader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Extract latent representations for analysis or visualization\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample_batch = next(iter(unique_loader))[0].to(device)\n",
        "    latent_codes = model.get_latent(sample_batch)\n",
        "    print(\"Latent representations shape:\", latent_codes.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLKneguZizB5",
        "outputId": "689fa70a-ef5d-4946-cee8-2050060499bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.4320\n",
            "Epoch [2/50], Loss: 0.1302\n",
            "Epoch [3/50], Loss: 0.1182\n",
            "Epoch [4/50], Loss: 0.1135\n",
            "Epoch [5/50], Loss: 0.1053\n",
            "Epoch [6/50], Loss: 0.0761\n",
            "Epoch [7/50], Loss: 0.0669\n",
            "Epoch [8/50], Loss: 0.0634\n",
            "Epoch [9/50], Loss: 0.0573\n",
            "Epoch [10/50], Loss: 0.0533\n",
            "Epoch [11/50], Loss: 0.0524\n",
            "Epoch [12/50], Loss: 0.0517\n",
            "Epoch [13/50], Loss: 0.0512\n",
            "Epoch [14/50], Loss: 0.0508\n",
            "Epoch [15/50], Loss: 0.0504\n",
            "Epoch [16/50], Loss: 0.0500\n",
            "Epoch [17/50], Loss: 0.0494\n",
            "Epoch [18/50], Loss: 0.0480\n",
            "Epoch [19/50], Loss: 0.0434\n",
            "Epoch [20/50], Loss: 0.0400\n",
            "Epoch [21/50], Loss: 0.0380\n",
            "Epoch [22/50], Loss: 0.0363\n",
            "Epoch [23/50], Loss: 0.0351\n",
            "Epoch [24/50], Loss: 0.0343\n",
            "Epoch [25/50], Loss: 0.0336\n",
            "Epoch [26/50], Loss: 0.0329\n",
            "Epoch [27/50], Loss: 0.0323\n",
            "Epoch [28/50], Loss: 0.0315\n",
            "Epoch [29/50], Loss: 0.0306\n",
            "Epoch [30/50], Loss: 0.0299\n",
            "Epoch [31/50], Loss: 0.0292\n",
            "Epoch [32/50], Loss: 0.0288\n",
            "Epoch [33/50], Loss: 0.0282\n",
            "Epoch [34/50], Loss: 0.0280\n",
            "Epoch [35/50], Loss: 0.0275\n",
            "Epoch [36/50], Loss: 0.0272\n",
            "Epoch [37/50], Loss: 0.0269\n",
            "Epoch [38/50], Loss: 0.0266\n",
            "Epoch [39/50], Loss: 0.0264\n",
            "Epoch [40/50], Loss: 0.0261\n",
            "Epoch [41/50], Loss: 0.0258\n",
            "Epoch [42/50], Loss: 0.0255\n",
            "Epoch [43/50], Loss: 0.0253\n",
            "Epoch [44/50], Loss: 0.0248\n",
            "Epoch [45/50], Loss: 0.0245\n",
            "Epoch [46/50], Loss: 0.0241\n",
            "Epoch [47/50], Loss: 0.0238\n",
            "Epoch [48/50], Loss: 0.0235\n",
            "Epoch [49/50], Loss: 0.0232\n",
            "Epoch [50/50], Loss: 0.0229\n",
            "Latent representations shape: torch.Size([256, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def compare_distances(model, dataset, device, sample_size=50):\n",
        "    # Randomly sample sample_size images from the dataset\n",
        "    indices = torch.randperm(len(dataset))[:sample_size]\n",
        "    sample = torch.stack([dataset[i][0] for i in indices]).to(device)  # shape: [sample_size, 784]\n",
        "\n",
        "    # Get latent representation for the sample\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        latent = model.get_latent(sample)  # shape: [sample_size, latent_dim]\n",
        "\n",
        "    # Compute pairwise Euclidean distances in the original space and latent space\n",
        "    dist_original = torch.cdist(sample, sample, p=2)  # Euclidean (L2) distance\n",
        "    dist_latent = torch.cdist(latent, latent, p=2)      # Euclidean (L2) distance\n",
        "\n",
        "    # Compute cosine similarities (convert to cosine distances if needed)\n",
        "    sample_norm = sample / sample.norm(dim=1, keepdim=True)\n",
        "    latent_norm = latent / latent.norm(dim=1, keepdim=True)\n",
        "    cosine_sim_original = torch.mm(sample_norm, sample_norm.t())\n",
        "    cosine_sim_latent = torch.mm(latent_norm, latent_norm.t())\n",
        "\n",
        "    # For a quick quantitative comparison, compute correlation between the flattened distance matrices\n",
        "    corr_euclidean = np.corrcoef(dist_original.cpu().numpy().flatten(),\n",
        "                                 dist_latent.cpu().numpy().flatten())[0, 1]\n",
        "    print(\"Correlation (Euclidean distances) between original and latent spaces:\", corr_euclidean)\n",
        "\n",
        "    corr_cosine = np.corrcoef(cosine_sim_original.cpu().numpy().flatten(),\n",
        "                              cosine_sim_latent.cpu().numpy().flatten())[0, 1]\n",
        "    print(\"Correlation (Cosine similarities) between original and latent spaces:\", corr_cosine)\n",
        "\n",
        "    return dist_original, dist_latent, cosine_sim_original, cosine_sim_latent\n",
        "\n",
        "# Example usage (assuming model, unique_dataset, and device are defined as in previous code):\n",
        "dist_original, dist_latent, cosine_sim_original, cosine_sim_latent = compare_distances(model, unique_dataset, device, sample_size=50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi3FSlXLjUja",
        "outputId": "369cc49c-f75e-459a-c188-136698e9942d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation (Euclidean distances) between original and latent spaces: 0.8127881534848016\n",
            "Correlation (Cosine similarities) between original and latent spaces: 0.863047631388183\n"
          ]
        }
      ]
    }
  ]
}